import random
import os
import re
import time
from datetime import datetime, date
import urllib.parse
import streamlit as st
from bs4 import BeautifulSoup
import csv
import os.path
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- æ–°ã—ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from curl_cffi import requests
from fake_useragent import UserAgent

# ==========================================
# ğŸª åº—èˆ—ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ (stores.txtå¯¾å¿œ)
# ==========================================
STORE_CONFIG_FILE = "stores.txt"
STORE_CONFIG = {}

def load_store_config():
    """stores.txt ã‹ã‚‰åº—èˆ—è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    config = {}
    if not os.path.exists(STORE_CONFIG_FILE):
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
        return {}
    
    try:
        with open(STORE_CONFIG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œ(#)ã‚„ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                if not line or line.startswith("#"):
                    continue
                
                # ãƒ‘ã‚¤ãƒ—(|)ã§3ã¤ã«åˆ†å‰²
                parts = line.split("|")
                if len(parts) >= 2:
                    name = parts[0].strip()
                    url = parts[1].strip()
                    # 3ã¤ã‚ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ï¼‰ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ã«ã™ã‚‹
                    event_text = parts[2].strip() if len(parts) > 2 else ""
                    
                    config[name] = {
                        "url": url,
                        "event_text": event_text
                    }
    except Exception as e:
        st.error(f"åº—èˆ—ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    return config

# èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚€
STORE_CONFIG = load_store_config()

# ==========================================
# ğŸ•’ åé›†ã®å®‰å…¨æ™‚é–“å¸¯ã‚¬ãƒ¼ãƒ‰
# ==========================================
SCRAPE_SAFE_START = (8, 0)   # 08:00
SCRAPE_SAFE_END   = (9, 59)  # 09:59

def is_safe_scrape_time(now_dt=None):
    if now_dt is None: now_dt = datetime.now()
    h, m = now_dt.hour, now_dt.minute
    start_h, start_m = SCRAPE_SAFE_START
    end_h, end_m     = SCRAPE_SAFE_END
    current_mins = h * 60 + m
    start_mins = start_h * 60 + start_m
    end_mins = end_h * 60 + end_m
    return start_mins <= current_mins <= end_mins

def safe_window_text():
    return f"{SCRAPE_SAFE_START[0]:02d}:{SCRAPE_SAFE_START[1]:02d}ã€œ{SCRAPE_SAFE_END[0]:02d}:{SCRAPE_SAFE_END[1]:02d}"

# ==========================================
# ğŸ¯ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹æ©Ÿç¨®ãƒªã‚¹ãƒˆ
# ==========================================
TARGET_KEYWORDS = [
    "ãƒã‚¤ã‚¸ãƒ£ã‚°", "ãƒã‚¤ã‚¸ãƒ£ã‚°V", "ãƒã‚¤ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ãƒ•ã‚¡ãƒ³ã‚­ãƒ¼", "ãƒ•ã‚¡ãƒ³ã‚­ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ã‚¢ã‚¤ãƒ ", "ã‚¢ã‚¤ãƒ ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼", "ã‚´ãƒ¼ã‚¸ãƒ£ã‚°", "ã‚´ãƒ¼ã‚´ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ãƒãƒƒãƒ”ãƒ¼", "ãƒãƒƒãƒ”ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ãƒ‡ã‚£ã‚¹ã‚¯", "DISK", "ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ãƒƒãƒ—",
    "åŒ—æ–—", "åŒ—æ–—ã®æ‹³", "ã‚¹ãƒã‚¹ãƒ­åŒ—æ–—"
]

PROXY_LIST_FILE = 'proxy_list.txt'

# ==========================================
# ğŸ›¡ï¸ æ¥ç¶šãƒ­ã‚¸ãƒƒã‚¯ (æœ€å¼·å½è£…)
# ==========================================
def get_random_ua():
    try: return UserAgent().random
    except: return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"

def load_proxies(filename=PROXY_LIST_FILE):
    if not os.path.exists(filename): return []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except: return []

def get_soup(url, max_retries=3):
    proxies_list = load_proxies()
    attempt_methods = [{"proxy": None, "type": "Direct"}]
    if proxies_list:
        sample = random.sample(proxies_list, min(len(proxies_list), 5))
        for p in sample: attempt_methods.append({"proxy": p, "type": "Proxy"})

    for attempt in attempt_methods:
        time.sleep(random.uniform(1.5, 4.0)) 
        headers = {
            "User-Agent": get_random_ua(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Referer": "https://www.google.com/",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "cross-site",
            "Sec-Ch-Ua": '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"Windows"'
        }
        proxies_dict = {"http": f"http://{attempt['proxy']}", "https": f"http://{attempt['proxy']}"} if attempt['proxy'] else None

        try:
            print(f"[{attempt['type']}] Fetching: {url[:40]}...")
            response = requests.get(url, headers=headers, proxies=proxies_dict, impersonate="chrome124", timeout=20)
            if response.status_code == 200:
                if "Just a moment" in response.text or "Cloudflare" in response.text:
                    print(f"âš ï¸ Cloudflare Block detected ({url})")
                    continue
                return BeautifulSoup(response.text, 'html.parser')
            else:
                print(f"âš ï¸ Status Code: {response.status_code} ({url})")
        except Exception as e:
            print(f"âš ï¸ Connection Error: {e}")
            continue
    return None

# ==========================================
# ãƒ‡ãƒ¼ã‚¿è§£æãƒ»ä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def fetch_machine_detail(url):
    soup = get_soup(url)
    if not soup: return {}
    detail_map = {}
    tables = soup.find_all('table')
    for table in tables:
        headers = [th.get_text(strip=True) for th in table.find_all('th')]
        if 'BB' in headers and 'RB' in headers:
            try:
                idx_num = next((i for i, h in enumerate(headers) if 'å°ç•ª' in h), -1)
                idx_bb = headers.index('BB')
                idx_rb = headers.index('RB')
                idx_total = next((i for i, h in enumerate(headers) if 'åˆ' in h), -1)
                if idx_num == -1: continue
                rows = table.find_all('tr')
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    cols_text = [ele.get_text(strip=True) for ele in cols]
                    if len(cols_text) > max(idx_num, idx_bb, idx_rb):
                        num = cols_text[idx_num]
                        if re.search(r'\d+', num):
                            detail_map[num] = {'BB': cols_text[idx_bb], 'RB': cols_text[idx_rb], 'åˆæˆ': cols_text[idx_total] if idx_total != -1 else '-'}
            except: continue
            break
    return detail_map

def process_extra_data(target_machines):
    extra_data_map = {}
    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_url = {executor.submit(fetch_machine_detail, m_url): m_name for m_name, m_url in target_machines}
        for future in as_completed(future_to_url):
            try: extra_data_map.update(future.result())
            except: pass
    return extra_data_map

def save_daily_data(detail_url, date_str, save_dir):
    if not is_safe_scrape_time():
        return False
    
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    filename = os.path.join(save_dir, f"{date_str}.csv")
    if os.path.exists(filename): return "EXIST"
    if not detail_url.startswith("http"): detail_url = "https://min-repo.com" + detail_url
    target_url = f"{detail_url}?kishu=all"

    soup = get_soup(target_url)
    if not soup: return False

    tables = soup.find_all('table')
    if not tables: return False
    main_data = []
    machine_link_map = {}
    headers = None

    for table in tables:
        header_row = table.find('tr')
        if not header_row: continue
        tmp_headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
        if not all(r in str(tmp_headers) for r in ['æ©Ÿç¨®', 'å°ç•ª', 'å·®æš']): continue
        headers = tmp_headers
        rows = table.find_all('tr')
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if not cols: continue
            cols_text = [ele.get_text(strip=True) for ele in cols]
            if cols_text == tmp_headers: continue
            link_ele = cols[0].find('a')
            if link_ele:
                m_name = cols_text[0] if len(cols_text) > 0 else ""
                full_link = urllib.parse.urljoin(target_url, link_ele.get('href'))
                if m_name: machine_link_map[m_name] = full_link
            main_data.append(cols_text)
        if main_data: break

    if not main_data: return False

    target_machines = []
    for m_name, m_url in machine_link_map.items():
        for kw in TARGET_KEYWORDS:
            if kw in m_name:
                target_machines.append((m_name, m_url))
                break
    extra_data_map = process_extra_data(target_machines) if target_machines else {}

    output_headers = headers + ['BB', 'RB', 'åˆæˆ']
    output_rows = [output_headers]
    try: idx_num = next(i for i, h in enumerate(headers) if 'å°ç•ª' in h)
    except: idx_num = 1

    for row in main_data:
        if len(row) > idx_num:
            num = row[idx_num]
            clean_num = re.sub(r'\D', '', str(num))
            if num in extra_data_map: ex = extra_data_map[num]; row_extended = row + [ex['BB'], ex['RB'], ex['åˆæˆ']]
            elif clean_num in extra_data_map: ex = extra_data_map[clean_num]; row_extended = row + [ex['BB'], ex['RB'], ex['åˆæˆ']]
            else: row_extended = row + ['-', '-', '-']
            output_rows.append(row_extended)
        else: output_rows.append(row + ['-', '-', '-'])

    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerows(output_rows)
        return True
    except: return False

# ==========================================
# ğŸš€ å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ==========================================
def run_scraping(store_name, start_date, end_date, max_workers=3): 
    # STORE_CONFIGã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã®å¤‰æ›´ã¯ä¸è¦
    store_info = STORE_CONFIG.get(store_name)
    if not store_info:
        st.error(f"åº—èˆ—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {store_name}")
        return

    save_dir = store_name 
    if not os.path.exists(save_dir): os.makedirs(save_dir)

    status_text = st.empty()
    progress_bar = st.progress(0)
    status_text.info(f"ğŸš€ {store_name} ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

    if not is_safe_scrape_time():
        status_text.error(f"â›” æ™‚é–“å¤–ã§ã™ ({safe_window_text()})ã€‚9:59ã‚’éããŸãŸã‚åé›†ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã€‚")
        progress_bar.empty()
        return

    current_url = store_info["url"]
    current_year = datetime.now().year
    last_month = 13
    target_tasks = [] 
    page_count = 1
    max_scan_pages = 25 

    while page_count <= max_scan_pages:
        if not is_safe_scrape_time():
            status_text.warning(f"â° æ™‚é–“ã‚ªãƒ¼ãƒãƒ¼ ({safe_window_text()})ï¼ é€²è¡Œä¸­ã®å‡¦ç†ãŒå®Œäº†æ¬¡ç¬¬ã€åœæ­¢ã—ã¾ã™...")
            break

        status_text.write(f"ğŸ” {store_name} ãƒªãƒ³ã‚¯æ¢ç´¢ä¸­... {page_count}ãƒšãƒ¼ã‚¸ç›®")
        soup = get_soup(current_url)
        if not soup: break
        all_links = soup.find_all('a')
        
        for link in all_links:
            text = link.get_text(strip=True)
            href = link.get('href')
            if not href: continue
            match = re.search(r'(\d{1,2})/(\d{1,2})', text)
            if match and re.search(r'\d+', href):
                m, d = int(match.group(1)), int(match.group(2))
                if not (1 <= m <= 12): continue
                if page_count > 1 and m > last_month and (last_month != 13): current_year -= 1
                last_month = m
                try: d_date = date(current_year, m, d)
                except: continue
                if d_date > end_date: continue 
                if d_date < start_date:
                    if (start_date - d_date).days > 7: page_count = 999; break
                    continue
                date_str = f"{current_year}-{m:02d}-{d:02d}"
                if not any(t[1] == date_str for t in target_tasks): target_tasks.append((href, date_str))

        if page_count == 999: break
        next_page = soup.find('a', class_='next') or soup.find('a', string=re.compile(r'æ¬¡|Next|next', re.I))
        if next_page and next_page.get('href'): current_url = next_page.get('href'); page_count += 1; time.sleep(1) 
        else: break

    total_tasks = len(target_tasks)
    if total_tasks == 0 and page_count != 999 and is_safe_scrape_time():
        status_text.warning(f"âš ï¸ {store_name}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    completed = 0
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_date = {executor.submit(save_daily_data, href, d_str, save_dir): d_str for href, d_str in target_tasks}
        for future in as_completed(future_to_date):
            d_str = future_to_date[future]
            try:
                res = future.result()
                if res: 
                    completed += 1
                prog = int((completed / total_tasks) * 100)
                progress_bar.progress(prog)
            except: pass

    if completed > 0:
        status_text.success(f"ğŸ‰ {store_name}: å®Œäº† ({completed}/{total_tasks})")
    elif not is_safe_scrape_time():
        status_text.warning("â° æ™‚é–“åˆ‡ã‚Œã®ãŸã‚ã€æœªå®Ÿè¡Œã®ã‚¿ã‚¹ã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
    
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()